{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "KVcLJT6jXqdV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLe05yXv7jQO",
        "outputId": "5c65c21b-ceff-43c7-c90d-cacea9a83786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uniseg in /usr/local/lib/python3.7/dist-packages (0.7.1.post2)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Imports\n",
        "'''\n",
        "!pip install uniseg\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import shutil\n",
        "#HTMl library to generate the connectivity html file\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfr4kTIE7wop",
        "outputId": "f9d1368d-b49d-43f0-c434-0cfc95504d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   228M      0  0:00:08  0:00:08 --:--:--  236M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Downloading the dataset\n",
        "'''\n",
        "# Download the dataset\n",
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "# Extract the downloaded tar file\n",
        "!tar -xvf  'daksh.tar'\n",
        "# Set the file paths to train, validation and test dataset\n",
        "#train_path\n",
        "train_file_path=os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.train.tsv\")\n",
        "#validation_path\n",
        "vaildation_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.dev.tsv\")\n",
        "#test_path\n",
        "test_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.test.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqZUg_I-81Sh"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Preprocessing the words\n",
        "'''\n",
        "def word_process(w):\n",
        "  #adding the tab and next line characters in the words\n",
        "  w = '\\t' + w + '\\n'\n",
        "  return w\n",
        "\n",
        "'''\n",
        "Function - Returns pairs of target word,input word.\n",
        "'''\n",
        "def create_dataset(path):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  #creating the word pairs\n",
        "  word_pairs = [[word_process(w)  for w in line.split('\\t')[:-1]]\n",
        "                for line in lines[:-1]]\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "'''\n",
        "Function - tokenize the language\n",
        "'''\n",
        "def tokenize(lang):\n",
        "  #using keras text tokenizer\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  #generating the sequence\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  #tensor used for pading the sequences\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  #retun the tensor and the language tokenizer\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "'''\n",
        "Function - load_dataset \n",
        "'''\n",
        "def load_dataset(path):\n",
        "  #creating the target word and input word pairs\n",
        "  output_lang, inp_lang = create_dataset(path)\n",
        "\n",
        "  #generating the tokenized tensor for the input words\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  #generating the tokenized tensor for the target words\n",
        "  output_tensor, output_lang_tokenizer = tokenize(output_lang)\n",
        "\n",
        "  return input_tensor, output_tensor, inp_lang_tokenizer, output_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTTPBoWe8j-x"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Reading the training dataset entirely\n",
        "'''\n",
        "# Use the entire training dataset file\n",
        "input_tensor_train, target_tensor_train, inp_lang, targ_lang = load_dataset(train_file_path)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor_train.shape[1], input_tensor_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aU5v5A68Raa"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class - GRU Encoder\n",
        "'''\n",
        "class GRU_Encoder(tf.keras.Model):\n",
        "  #Initialization\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout=0):\n",
        "    super(GRU_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch_size\n",
        "    self.enc_units = enc_units  #encoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embeding dimensions and layer initialization\n",
        "    #keras GRU layer\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = dropout)\n",
        "  #calling the GRU encoder\n",
        "  def call(self, x, hidden):\n",
        "    #calling the embeding initializations\n",
        "    x = self.embedding(x)\n",
        "    #return the encoder output and the state\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  #initialiaztion of the hidden states\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "'''\n",
        "class - LSTM encoder\n",
        "'''\n",
        "class LSTM_Encoder(tf.keras.Model):\n",
        "  #initialization function\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz,dropout=0):\n",
        "    super(LSTM_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch_size\n",
        "    self.enc_units = enc_units  #encoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embedding_dimensions\n",
        "    #keras LSTM layer\n",
        "    self.lstm = tf.keras.layers.LSTM(self.enc_units, \n",
        "                         return_sequences=True, \n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "\n",
        "  #call function\n",
        "  def call(self, x, hidden,cell_state):\n",
        "    #embedding layer calling\n",
        "    x = self.embedding(x)\n",
        "    #output and the last cell calling\n",
        "    output, last_hidden,last_cell_state = self.lstm(x, initial_state=[hidden,cell_state])\n",
        "    #return the output, last hidden and the last cell state\n",
        "    return output, last_hidden,last_cell_state\n",
        "    \n",
        "  #initialization of the hidden state\n",
        "  def initialize_hidden_state(self):\n",
        "      return tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "'''\n",
        "class - RNN encoder\n",
        "'''\n",
        "class RNN_Encoder(tf.keras.Model):\n",
        "  #intialization function\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz,dropout=0):\n",
        "    super(RNN_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch size\n",
        "    self.enc_units = enc_units  #encoder units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)   #embedding dimensions\n",
        "    #keras RNN layer\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(self.enc_units, \n",
        "                         return_sequences=True, \n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "\n",
        "  #call function\n",
        "  def call(self, x, hidden):\n",
        "    #embedding layer calling\n",
        "    x = self.embedding(x)\n",
        "    #returning the output and the final state\n",
        "    output, final_state = self.rnn(x,initial_state=hidden)\n",
        "    return output, final_state\n",
        "    \n",
        "  #initialization of the hidden states\n",
        "  def initialize_hidden_state(self):\n",
        "      return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3C9skWL8-If"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Attention class (Bhadanau Attention) refernce for the attention  - https://arxiv.org/abs/1409.0473\n",
        "'''\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  #initialization\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)  #W_1\n",
        "    self.W2 = tf.keras.layers.Dense(units)  #W_2\n",
        "    self.V = tf.keras.layers.Dense(1)       #V\n",
        "\n",
        "  '''\n",
        "  call function genrating the context vector and the attention weights\n",
        "  '''\n",
        "  def call(self, query, values):\n",
        "    '''\n",
        "    shape of query hidden state == (batch_size, hidden size)\n",
        "    shape of query_with_time_axis == (batch_size, 1, hidden size)\n",
        "    shape of values  == (batch_size, max_len, hidden size)\n",
        "    '''\n",
        "    #To broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # shape of score == (batch_size, max_length, 1)\n",
        "    # shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # shape of attention_weights == (batch_size, max_length, 1)\n",
        "    #generating the attention weights\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    #generating the context vector\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    #returning the context vector and the attention weights\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snk7YcBD9AQy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class GRU Decoder \n",
        "'''\n",
        "class GRU_Decoder(tf.keras.Model):\n",
        "  #initialization\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(GRU_Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch size\n",
        "    self.dec_units = dec_units  #decoder units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embeding layer initialization\n",
        "    #keras GRU layer\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',dropout=dropout)\n",
        "    #dense or fully connected layer\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    #using the attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  #call function to generate the output, state and the attention weights\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # output shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    output = self.embedding(x)\n",
        "    # output shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(output)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    output = self.fc(output)\n",
        "    #return the output, state and the attention weights.\n",
        "    return output, state, attention_weights\n",
        "\n",
        "'''\n",
        "class LSTM decoder\n",
        "'''\n",
        "class LSTM_Decoder(tf.keras.Model):\n",
        "  #initialization\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(LSTM_Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch size\n",
        "    self.dec_units = dec_units  #decoder units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embedding\n",
        "    #keras LSTM layer\n",
        "    self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',dropout=dropout)\n",
        "    #dense/ Fully connected layer\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    #applying the attention layer\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  #call function generating output, hiddden and cell state and the attention weights\n",
        "  def call(self, x, hidden, enc_output,cell_state):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    output = self.embedding(x)\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, last_hidden_state,last_cell_state = self.lstm(output,initial_state=[hidden,cell_state])\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    output = self.fc(output)\n",
        "    #returning output, hiddden and cell state and the attention weights\n",
        "    return output, [last_hidden_state,last_cell_state], attention_weights\n",
        "\n",
        "'''\n",
        "Class RNN decoder\n",
        "'''\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "  #initialization\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch size\n",
        "    self.dec_units = dec_units  #decoder unnits\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embedding layer\n",
        "    #keras RNN layer\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(self.dec_units, \n",
        "                         return_sequences=True, \n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "    #dense/ fully connected layer\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    #applying attention layer\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "\n",
        "  #call function generating the output state and the attention weights\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    output = self.embedding(x)\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, final_state = self.rnn(output)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    output = self.fc(output)\n",
        "    #return the output state and the attention weights\n",
        "    return output, final_state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPS--UX69DAJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Fucntion - Calculating the loss function\n",
        "Reference: https://stackoverflow.com/questions/62916592/loss-function-for-sequences-in-tensorflow-2-0\n",
        "'''\n",
        "def calculate_loss(real, pred):\n",
        "  mask_position = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_value = loss_object(real, pred)\n",
        "\n",
        "  mask_position = tf.cast(mask_position, dtype=loss_value.dtype)\n",
        "  loss_value *= mask_position\n",
        "\n",
        "  #returns the mean of the loss value\n",
        "  return tf.reduce_mean(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout,train_input_batch):\n",
        "    #Encoder\n",
        "    if rnn_type=='GRU': #GRU\n",
        "       encoder = GRU_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "       sample_hidden = encoder.initialize_hidden_state()\n",
        "       sample_output, sample_hidden = encoder(train_input_batch, sample_hidden)\n",
        "    elif rnn_type=='LSTM': #LSTM\n",
        "      encoder = LSTM_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_hidden,sample_cell_state = encoder.initialize_hidden_state()\n",
        "      sample_output, sample_hidden,sample_cell_state = encoder(train_input_batch, sample_hidden,sample_cell_state)\n",
        "    elif rnn_type=='RNN': #RNN\n",
        "      encoder = RNN_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_hidden = encoder.initialize_hidden_state()\n",
        "      sample_output, sample_hidden = encoder(train_input_batch, sample_hidden)\n",
        "    #printing the shapes\n",
        "    print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
        "    print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)\n",
        "    \n",
        "    #Decoder\n",
        "    if rnn_type=='GRU': #GRU\n",
        "      decoder = GRU_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "    \n",
        "    elif rnn_type=='LSTM': #LSTM\n",
        "      decoder = LSTM_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output, sample_cell_state)\n",
        "    \n",
        "    elif rnn_type=='RNN': #RNN\n",
        "      decoder = RNN_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "      \n",
        "    #print the decoder shape\n",
        "    print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
        "\n",
        "    return encoder,decoder"
      ],
      "metadata": {
        "id": "xGUVdtvkNyTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epochs(EPOCHS,encoder,decoder,dataset,steps_per_epoch):\n",
        "    train_loss=[0]*EPOCHS\n",
        "    for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "      if rnn_type!='LSTM': #GRU or RNN\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "      elif rnn_type=='LSTM': #LSTM\n",
        "        enc_hidden,enc_cell_state = encoder.initialize_hidden_state()\n",
        "      total_loss = 0\n",
        "      for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        #Training for every batch\n",
        "        if rnn_type!='LSTM':\n",
        "          batch_loss = train_batch(inp, targ, enc_hidden, encoder,decoder,rnn_type)\n",
        "        elif rnn_type=='LSTM':\n",
        "          batch_loss = train_batch(inp, targ, [enc_hidden,enc_cell_state], encoder,decoder,rnn_type)\n",
        "        total_loss += batch_loss\n",
        "      if batch % 100 == 0:\n",
        "        #printing the batch loss\n",
        "        print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "        \n",
        "      # saving (checkpoint) the model every 2 epochs\n",
        "      if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "      #print the trtaining loss for the epoch\n",
        "      print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "      print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "      # Storing the average loss per epoch\n",
        "      train_loss[epoch] = total_loss.numpy()/steps_per_epoch\n",
        "\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "t0hlWYxWaq9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEuZduf39E2J"
      },
      "outputs": [],
      "source": [
        "def train(use_wandb=True):\n",
        "    global BATCH_SIZE \n",
        "    global units \n",
        "    global vocab_inp_size\n",
        "    global vocab_tar_size\n",
        "    global embedding_dim\n",
        "    global encoder\n",
        "    global decoder\n",
        "    global optimizer\n",
        "    global loss_object\n",
        "    global checkpoint_dir\n",
        "    global checkpoint_prefix \n",
        "    global checkpoint\n",
        "    global run_name\n",
        "    global rnn_type\n",
        "\n",
        "\n",
        "    '''\n",
        "    Wandb configuration\n",
        "    '''\n",
        "    # initialising the wandb run\n",
        "    run = wandb.init()\n",
        "    # Tpye of RNN to choose. Acceptable Values are 'RNN'. 'LSTM' and 'GRU'\n",
        "    rnn_type = run.config.rnn_type\n",
        "    # Batch size for training.\n",
        "    BATCH_SIZE = run.config.bs\n",
        "    # Dimensions of the abstract representation of the input word and target word.\n",
        "    embedding_dim = run.config.embed\n",
        "    # Latent dimensions of the encoder and decoder.\n",
        "    units = run.config.latent\n",
        "    # Number of epochs to train for.\n",
        "    EPOCHS = run.config.epochs\n",
        "    #\tFloat between 0 and 1. Denotes the fraction of the units to drop.\n",
        "    dropout = run.config.dropout\n",
        "\n",
        "\n",
        "    print(\"rnn_Type: \",rnn_type)\n",
        "    #buffer size\n",
        "    BUFFER_SIZE = len(input_tensor_train)\n",
        "    #steps per epoch\n",
        "    steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "    #vocab input size\n",
        "    vocab_inp_size = len(inp_lang.word_index)+1\n",
        "    #vocab target size\n",
        "    vocab_tar_size = len(targ_lang.word_index)+1\n",
        "    \n",
        "    #wandb run name\n",
        "    run_name = '_epochs_'+str(EPOCHS)+'_rnn_type_'+str(rnn_type)+'_bs_'+str(BATCH_SIZE)+'_embed_'+str(embedding_dim)+'_latent_'+str(units)+'_dropout_'+str(dropout)\n",
        "    if use_wandb==True:\n",
        "      wandb.run.name = run_name\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "    # We create batches of size BATCH_SIZE and ignore the last batch because the last batch may not be equal to BATCH_SIZE\n",
        "    #creating the batches\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    \n",
        "    train_input_batch, train_target_batch = next(iter(dataset))\n",
        "    \n",
        "    encoder, decoder = build_model(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout,train_input_batch)\n",
        "  \n",
        "    #Apply the adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    \n",
        "    #saving the checkpoints\n",
        "    checkpoint_dir = os.path.join(os.getcwd(),'training_checkpoints')\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "        \n",
        "    ####################################################################### Hyper parameter Tuning (Training) ###############################################################\n",
        "    \n",
        "    train_loss=[0]*EPOCHS\n",
        "    for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "      if rnn_type!='LSTM': #GRU or RNN\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "      elif rnn_type=='LSTM': #LSTM\n",
        "        enc_hidden,enc_cell_state = encoder.initialize_hidden_state()\n",
        "      total_loss = 0\n",
        "      for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        #Training for every batch\n",
        "        if rnn_type!='LSTM':\n",
        "          batch_loss = train_batch(inp, targ, enc_hidden, encoder,decoder,rnn_type)\n",
        "        elif rnn_type=='LSTM':\n",
        "          batch_loss = train_batch(inp, targ, [enc_hidden,enc_cell_state], encoder,decoder,rnn_type)\n",
        "        total_loss += batch_loss\n",
        "      if batch % 100 == 0:\n",
        "        #printing the batch loss\n",
        "        print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "        \n",
        "      # saving (checkpoint) the model every 2 epochs\n",
        "      if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "      #print the trtaining loss for the epoch\n",
        "      print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "      print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "      # Storing the average loss per epoch\n",
        "      train_loss[epoch] = total_loss.numpy()/steps_per_epoch\n",
        "      #logging the training loss in wandb\n",
        "      if use_wandb == True:\n",
        "        wandb.log({\"train_loss\": total_loss.numpy()/steps_per_epoch})\n",
        "\n",
        "        \n",
        "    #test_accuracy = validate(test_file_path,run_name)\n",
        "    val_acc=validate(vaildation_file_path,rnn_type)\n",
        "    print(\"Train loss: \",train_loss)\n",
        "    print(\"Validation Accuracy: \",val_acc)\n",
        "    #print(\"Test Accuracy: \",test_accuracy)\n",
        "    if use_wandb ==True:\n",
        "      wandb.log({'val_accuracy': val_acc})\n",
        "    \n",
        "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQW3p5ETAFCu"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function for training manually the best configuration of the model\n",
        "'''\n",
        "def manual_train():\n",
        "    global BATCH_SIZE \n",
        "    global units \n",
        "    global vocab_inp_size\n",
        "    global vocab_tar_size\n",
        "    global embedding_dim\n",
        "    global encoder\n",
        "    global decoder\n",
        "    global optimizer\n",
        "    global loss_object\n",
        "    global checkpoint_dir\n",
        "    global checkpoint_prefix \n",
        "    global checkpoint\n",
        "    global run_name\n",
        "    global rnn_type\n",
        "\n",
        "    '''\n",
        "    Best configuration of the model\n",
        "    '''\n",
        "    rnn_type = 'LSTM'\n",
        "    BATCH_SIZE = 64\n",
        "    embedding_dim = 512\n",
        "    units = 1024\n",
        "    EPOCHS = 20\n",
        "    dropout = 0.2\n",
        "\n",
        "    print(\"rnn_Type: \",rnn_type)\n",
        "    #generating the buffer size\n",
        "    BUFFER_SIZE = len(input_tensor_train)\n",
        "    #calculating the number of steps per epoch\n",
        "    steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "    #vocab input size\n",
        "    vocab_inp_size = len(inp_lang.word_index)+1\n",
        "    #vocab target size\n",
        "    vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "    run_name = '_epochs_'+str(EPOCHS)+'_rnn_type_'+str(rnn_type)+'_bs_'+str(BATCH_SIZE)+'_embed_'+str(embedding_dim)+'_latent_'+str(units)+'_dropout_'+str(dropout)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "    #We are creating batches of size BATCH_SIZE and ignore the last batch because the last batch may not be equal to BATCH_SIZE\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    \n",
        "    train_input_batch, train_target_batch = next(iter(dataset))\n",
        "    \n",
        "    encoder, decoder = build_model(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout,train_input_batch)\n",
        "\n",
        "    #apply the adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    \n",
        "    #creating and saving the checkpoints\n",
        "    checkpoint_dir = os.path.join(os.getcwd(),'training_checkpoints')\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "    \n",
        "    train_loss=[0]*EPOCHS\n",
        "    \n",
        "    ############################################################################## Training ##########################################################################\n",
        "\n",
        "    train_loss = train_epochs(EPOCHS,encoder,decoder,dataset,steps_per_epoch)\n",
        "\n",
        "        \n",
        "    #calculating the test accuracy using the validate function\n",
        "    test_accuracy = validate(test_file_path,run_name)\n",
        "    #calcualting the validation accuracy using validate function\n",
        "    val_acc=validate(vaildation_file_path,rnn_type)\n",
        "    print(\"Train loss: \",train_loss)\n",
        "    print(\"Validation Accuracy: \",val_acc)\n",
        "    print(\"Test Accuracy: \",test_accuracy)\n",
        "    \n",
        " \t  # restoring the latest checkpoint in checkpoint_dir and starting the test\n",
        "  \t# checkpoints are only useful when source code that will use the saved parameter values is available.\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "    generate_inputs(rnn_type,10)\n",
        "    #generating the connectivity of the best model.\n",
        "    connectivity(['maryaadaa','prayogshala','angarakshak'],rnn_type, os.path.join(os.getcwd(),\"predictions_attention\",str(run_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqfHJ7e59Kpr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function - train_batch\n",
        "calculates the loss of the batch and returns the batch loss after training every batch in each epoch\n",
        "'''\n",
        "@tf.function\n",
        "def train_batch(inp, targ, enc_hidden, enocder, decoder,rnn_type):\n",
        "  #Final loss value\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "        #checking if it is GRU or RNN\n",
        "        if rnn_type!='LSTM':\n",
        "            enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "            dec_hidden = enc_hidden\n",
        "        #checking if it is LSTM\n",
        "        elif rnn_type=='LSTM':\n",
        "            enc_output, enc_hidden,enc_cell_state = encoder(inp, enc_hidden[0],enc_hidden[1])\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_cell_state=enc_cell_state\n",
        "        \n",
        "        #geting the decoder input\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['\\t']] * BATCH_SIZE, 1)\n",
        "        \n",
        "        #Teacher forcing\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            if rnn_type!='LSTM':\n",
        "                # passing enc_output to the decoder if it is RNN or GRU\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            elif rnn_type=='LSTM':\n",
        "                if t==1:\n",
        "                  # passing enc_output to the decoder if it is a LSTM\n",
        "                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output,dec_cell_state)\n",
        "                elif t>1:\n",
        "                  # passing enc_output to the decoder if it is a LSTM\n",
        "                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden[0], enc_output,dec_cell_state)\n",
        "            #calculating the loss using calculate loss function\n",
        "            loss += calculate_loss(targ[:, t], predictions)\n",
        "            #using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  #calculating the batch loss\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  #calculate the variables\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  #calculate the gradients\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  #applying the gradients to the optimizer\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KqJkihJ9PT4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function - inference_model\n",
        "generating the predicted word, input word, attention weights and the attention plot\n",
        "'''\n",
        "def inference_model(input_word,rnn_type):\n",
        "  #creating an empty attention plot\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  #preprocessing the input word\n",
        "  input_word = word_process(input_word)\n",
        "\n",
        "  #converting the word to tensor after pading\n",
        "  inputs = [inp_lang.word_index[i] for i in input_word]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  #predicted word initialization\n",
        "  predicted_word = ''\n",
        "  \n",
        "  #if cell type is GRU or RNN\n",
        "  if rnn_type!='LSTM':\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "  #if cell type is LSTM\n",
        "  elif rnn_type=='LSTM':\n",
        "    hidden=tf.zeros((1, units))\n",
        "    cell_state= tf.zeros((1, units)) \n",
        "    enc_out, enc_hidden,enc_cell_state = encoder(inputs, hidden,cell_state)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "  #generating the decode inputs\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['\\t']], 0)\n",
        "\n",
        "  #storing the attention weights\n",
        "  att_w=[]\n",
        "\n",
        "  #calculating the predictions\n",
        "  for t in range(max_length_targ):\n",
        "    #if cell is GRU or RNN\n",
        "    if rnn_type!='LSTM':\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n",
        "    #if cell is LSTM\n",
        "    elif rnn_type=='LSTM':\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out, enc_cell_state)\n",
        "      dec_hidden=dec_hidden[0]\n",
        "\n",
        "    # storing the attention weights for plotting latter\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    att_w.append(attention_weights.numpy()[0:len(input_word)])\n",
        "    \n",
        "\n",
        "    #predicted id\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    #predicted word\n",
        "    predicted_word += targ_lang.index_word[predicted_id] \n",
        "\n",
        "    #in case of last character\n",
        "    if targ_lang.index_word[predicted_id] == '\\n':\n",
        "      return predicted_word, input_word, attention_plot,att_w\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "  #finally return the predicted word, input word, attention plot and the attention weight\n",
        "  return predicted_word, input_word, attention_plot,att_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVHrrxwt9Ro5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function - Validate\n",
        "returning the validation or the testing accuracy on the validation data\n",
        "'''\n",
        "def validate(path_to_file,folder_name):\n",
        "  #while testing to generate the predictions files (output files)\n",
        "  save = False\n",
        "  if path_to_file.find(\"test\")!=-1:\n",
        "    if os.path.exists(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name))):\n",
        "      shutil.rmtree(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name)))\n",
        "      \n",
        "    if not os.path.exists(os.path.join(os.getcwd(),\"predictions_attention\")):\n",
        "        os.mkdir(os.path.join(os.getcwd(),\"predictions_attention\"))\n",
        "    os.mkdir(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name)))\n",
        "    success_file = open(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name),\"success.txt\"),\"w\",encoding='utf-8', errors='ignore')\n",
        "    failure_file = open(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name),\"failure.txt\"),\"w\",encoding='utf-8', errors='ignore')\n",
        "    save=True\n",
        "    \n",
        "  #the count of the correct predictions\n",
        "  success_count=0\n",
        "  # Get the target words and input words for the validation\n",
        "  target_words, input_words = create_dataset(path_to_file)\n",
        "  for i in range(len(input_words)):\n",
        "    #generate the predicted words for the corresponding input words\n",
        "    predicted_word, input_word, attention_plot,att_w = inference_model(input_words[i],rnn_type)\n",
        "    record= input_word.strip()+' '+target_words[i].strip()+' '+predicted_word[:-1].strip()+\"\\n\"\n",
        "    # The last character of target_words[i] and predicted word is '\\n', first character of target_words[i] is '\\t'\n",
        "    if target_words[i][1:]==predicted_word:\n",
        "      #increasing the accuracy count\n",
        "      success_count = success_count + 1\n",
        "      if save == True:\n",
        "        success_file.write(record)\n",
        "    elif save==True:\n",
        "      failure_file.write(record)\n",
        "\n",
        "  #saving the files\n",
        "  if save==True:\n",
        "    success_file.close()\n",
        "    failure_file.close()\n",
        "    \n",
        "  #return the acuracy\n",
        "  return success_count/len(input_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoLXeHzI9VBr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function  - plot - attention\n",
        "Function ploting the attention plots.\n",
        "'''\n",
        "def plot_attention(attention, input_word, predicted_word, file_name):\n",
        "  #loading the hindi font for displaying\n",
        "  hindi_font = FontProperties(fname = os.path.join(os.getcwd(),\"Nirmala.ttf\"))\n",
        "  \n",
        "  #figure matplotlib\n",
        "  fig = plt.figure(figsize=(3, 3))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "  \n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + list(input_word), fontdict=fontdict, rotation=0)\n",
        "  ax.set_yticklabels([''] + list(predicted_word), fontdict=fontdict,fontproperties=hindi_font)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  #save the plot figure.\n",
        "  plt.savefig(file_name)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJHKHIcC9ijI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Geting the connectivity html file.\n",
        "'''\n",
        "# get html element\n",
        "def cstr(s, color='black'):\n",
        "\tif s == ' ':\n",
        "\t\treturn \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "\telse:\n",
        "\t\treturn \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\t\n",
        "# print html\n",
        "def print_color(t):\n",
        "\tdisplay(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "# get appropriate color for value\n",
        "# Darker shades of green denotes higher importance.\n",
        "def get_clr(value):\n",
        "\tcolors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n",
        "\t\t'#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "\t\t'#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "\t\t'#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "\tvalue = int((value * 100) / 5)\n",
        "\treturn colors[value]\n",
        "\n",
        "\n",
        "'''\n",
        "Function - Visualize the connectivity plots in the HTMl file\n",
        "'''\n",
        "def visualize(input_word, output_word, att_w):\n",
        "  for i in range(len(output_word)):\n",
        "    print(\"\\nOutput character:\", output_word[i], \"\\n\")\n",
        "    text_colours = []\n",
        "    for j in range(len(att_w[i])):\n",
        "      text = (input_word[j], get_clr(att_w[i][j]))\n",
        "      text_colours.append(text)\n",
        "    print_color(text_colours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvyqHnYQ9ka_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code for connectivity visualisation.\n",
        "'''\n",
        "# get appropriate color for value\n",
        "# Darker shades of green denotes higher importance.\n",
        "def get_shade_color(value):\n",
        "\tcolors = ['#00fa00', '#00f500',  '#00eb00', '#00e000',  '#00db00',  \n",
        "           '#00d100',  '#00c700',  '#00c200', '#00b800',  '#00ad00',  \n",
        "           '#00a800',  '#009e00',  '#009400', '#008f00',  '#008500',\n",
        "           '#007500',  '#007000',  '#006600', '#006100',  '#005c00',  \n",
        "           '#005200',  '#004d00',  '#004700', '#003d00',  '#003800',  \n",
        "           '#003300',  '#002900',  '#002400',  '#001f00',  '#001400']\n",
        "\tvalue = int((value * 100) / 5)\n",
        "\treturn colors[value]\n",
        "\n",
        "#creating the HTMl file\n",
        "def create_file(text_colors,input_word,output_word,file_path=os.getcwd()):\n",
        "  text = '''\n",
        "  <!DOCTYPE html>\n",
        "  <html>\n",
        "  <head>\n",
        "    <meta charset=\"UTF-8\"> \n",
        "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
        "    <script>\n",
        "            $(document).ready(function(){\n",
        "            var col =['''\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word)):\n",
        "              text=text+'''['''\n",
        "              for j in range(len(text_colors[k][i])-1):\n",
        "                text=text+'''\\\"'''+text_colors[k][i][j]+'''\\\"'''+''','''\n",
        "              text=text+'''\\\"'''+text_colors[k][i][len(text_colors[k][i])-1]+'''\\\"'''+'''],'''\n",
        "  text=text[0:-1]\n",
        "  text=text+'''];\\n'''\n",
        "  \n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word[k])):\n",
        "            text=text+'''$(\\\".h'''+str(k)+str(i)+'''\\\").mouseover(function(){\\n'''\n",
        "            for j in range(len(input_word[k])):\n",
        "                       text=text+'''$(\\\".t'''+str(k)+str(j)+'''\\\").css(\\\"background-color\\\", col['''+str(i)+''']'''+'''['''+str(j)+''']);\\n'''\n",
        "            text=text+'''});\\n'''\n",
        "            text=text+'''$(\\\".h'''+str(k)+str(i)+'''\\\").mouseout(function(){\\n'''\n",
        "            for l in range(3):\n",
        "              for j in range(len(input_word[l])):\n",
        "                text=text+'''$(\\\".t'''+str(l)+str(j)+'''\\\").css(\\\"background-color\\\", \\\"#ffff99\\\");\\n'''\n",
        "            text=text+'''});\\n'''\n",
        "  text=text+'''});\\n\n",
        "</script>\n",
        "  </head>\n",
        "      <body>\n",
        "          <h1>Connectivity:</h1>\n",
        "          <p> The connection strength between the target for the selected character and the input characters is highlighted in green (reset). Hover over the text to change the selected character.</p>\n",
        "          <div style=\"background-color:#ffff99;color:black;padding:2%; margin:4%;\">\n",
        "          <p>\n",
        "          <div> Output: </div>\n",
        "          <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>\n",
        "          '''\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word[k])):\n",
        "            text=text+'''\\n'''+'''\\t'''+'''<div class=\"h'''+str(k)+str(i)+'''\\\">'''+output_word[k][i]+'''</div>'''\n",
        "      text=text+'''</div>'''+'\\n'+'\\t'+'''<div>  </p>'''+'\\n'+'\\t'+'''<p>\n",
        "      <div> Input: </div>\n",
        "      <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>'''    \n",
        "      for j in range(len(input_word[k])):\n",
        "        text=text+'''\\n'''+'''\\t'''+'''<div class=\"t'''+str(k)+str(j)+'''\\\">'''+input_word[k][j]+'''</div>'''\n",
        "      if k<2:\n",
        "          text = text+'''</div></p></div><p></p></div>\n",
        "          <div style=\"background-color:#ffff99;color:black;padding:2%; margin:4%;\">\n",
        "          <div> Output: </div>\n",
        "          <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>'''\n",
        "  text=text+'''\n",
        "        </div>\n",
        "        </p>\n",
        "        </div>\n",
        "        </body>\n",
        "  </html>\n",
        "  '''\n",
        "  fname = os.path.join(file_path,\"connectivity.html\")\n",
        "  file = open(fname,\"w\")\n",
        "  file.write(text)\n",
        "  file.close()\n",
        "\n",
        "#main file to generate the connectivity of HTML file\n",
        "def connectivity(input_words,rnn_type,file_path):\n",
        "  #color list\n",
        "  color_list=[]\n",
        "  #input word list\n",
        "  input_word_list=[]\n",
        "  #output word list\n",
        "  output_word_list=[]\n",
        "\n",
        "  for k in range(3):\n",
        "    #do inferencing in the model\n",
        "    output_word, input_word, _ ,att_w = inference_model(input_words[k],rnn_type)\n",
        "    text_colours=[]\n",
        "    for i in range(len(output_word)):\n",
        "      colour=[]\n",
        "      for j in range(len(att_w[i])):\n",
        "        value=get_shade_color(att_w[i][j])\n",
        "        colour.append(value)\n",
        "      text_colours.append(colour)\n",
        "    #creating the color list\n",
        "    color_list.append(text_colours)\n",
        "    #input word list\n",
        "    input_word_list.append(input_word)\n",
        "    #output word list\n",
        "    output_word_list.append(output_word)\n",
        "  #create file for generating the HTML file\n",
        "  create_file(color_list,input_word_list,output_word_list,file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scx1WsVh9mEV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function - Transliteration\n",
        "generating the attention heatmap\n",
        "'''\n",
        "def transliterate(input_word,rnn_type,file_name=os.path.join(os.getcwd(),\"attention_heatmap.png\"),visual_flag=True):\n",
        "  #do inferencing ot get the predicted word and other attention plots, weights and input word\n",
        "  predicted_word, input_word, attention_plot,att_w = inference_model(input_word,rnn_type)\n",
        "\n",
        "  #geting the predicted transliterations\n",
        "  print(\"\\n\",'Input:', input_word)\n",
        "  print('Predicted transliteration:', predicted_word)\n",
        "\n",
        "  attention_plot = attention_plot[:len(predicted_word),\n",
        "                                  :len(input_word)]\n",
        "  plot_attention(attention_plot, input_word, predicted_word, file_name)\n",
        "\n",
        "  if visual_flag == True:\n",
        "    #visualize the attention plots\n",
        "    visualize(input_word, predicted_word, att_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxsCx9iU9n6m"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Fucntion - generate_inputs\n",
        "generate the predictions for the attention based model\n",
        "'''\n",
        "def generate_inputs(rnn_type,n_test_samples=10):\n",
        "  target_words, input_words = create_dataset(test_file_path)\n",
        "  \n",
        "  for i in range (n_test_samples):\n",
        "    index = random.randint(0,len(input_words))\n",
        "    input_word=input_words[index]\n",
        "    file_name=os.path.join(os.getcwd(),\"predictions_attention\",str(run_name),input_word+\".png\")\n",
        "    \n",
        "    if i == 0:\n",
        "      transliterate(input_word[1:-1],rnn_type, file_name,True)\n",
        "    elif i > 0:\n",
        "      transliterate(input_word[1:-1],rnn_type, file_name,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRKjcUDR-r1F"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Sweep configuration for hyper parameter tuning\n",
        "'''\n",
        "!pip install wandb --upgrade\n",
        "import wandb\n",
        "!wandb login\n",
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep without attention\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \n",
        "        \"rnn_type\": {\"values\": [\"RNN\",\"GRU\",\"LSTM\"]},\n",
        "        \n",
        "        \"embed\": {\"values\": [256,512]},\n",
        "        \n",
        "        \"latent\": {\"values\": [512,1024]},\n",
        "        \n",
        "        \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n",
        "\n",
        "        \"epochs\": {\"values\": [20]},\n",
        "        \n",
        "        \"bs\": {\"values\": [64]},\n",
        "\n",
        "\n",
        "    },\n",
        "  }\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_Assignment3_attention\", entity=\"cs21m007_cs21m013\")\n",
        "\n",
        "wandb.agent(sweep_id, train, count = 30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "manual training for the best parameter model\n",
        "'''\n",
        "manual_train()"
      ],
      "metadata": {
        "id": "Q3qXCUCWPPRI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2dfd78fe-c2b0-4677-b060-153f10254525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn_Type:  LSTM\n",
            "Encoder output shape: (batch size, sequence length, units) (64, 22, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Decoder output shape: (batch_size, vocab size) (64, 65)\n",
            "Epoch 1 Loss 0.3846\n",
            "Time taken for 1 epoch 149.47 sec\n",
            "\n",
            "Epoch 2 Loss 0.1350\n",
            "Time taken for 1 epoch 114.57 sec\n",
            "\n",
            "Epoch 3 Loss 0.1000\n",
            "Time taken for 1 epoch 116.41 sec\n",
            "\n",
            "Epoch 4 Loss 0.0779\n",
            "Time taken for 1 epoch 117.66 sec\n",
            "\n",
            "Epoch 5 Loss 0.0604\n",
            "Time taken for 1 epoch 116.97 sec\n",
            "\n",
            "Epoch 6 Loss 0.0492\n",
            "Time taken for 1 epoch 117.78 sec\n",
            "\n",
            "Epoch 7 Loss 0.0375\n",
            "Time taken for 1 epoch 116.92 sec\n",
            "\n",
            "Epoch 8 Loss 0.0334\n",
            "Time taken for 1 epoch 117.67 sec\n",
            "\n",
            "Epoch 9 Loss 0.0298\n",
            "Time taken for 1 epoch 117.01 sec\n",
            "\n",
            "Epoch 10 Loss 0.0275\n",
            "Time taken for 1 epoch 117.36 sec\n",
            "\n",
            "Epoch 11 Loss 0.0255\n",
            "Time taken for 1 epoch 117.16 sec\n",
            "\n",
            "Epoch 12 Loss 0.0258\n",
            "Time taken for 1 epoch 117.30 sec\n",
            "\n",
            "Epoch 13 Loss 0.0258\n",
            "Time taken for 1 epoch 117.11 sec\n",
            "\n",
            "Epoch 14 Loss 0.0225\n",
            "Time taken for 1 epoch 117.44 sec\n",
            "\n",
            "Epoch 15 Loss 0.0219\n",
            "Time taken for 1 epoch 116.97 sec\n",
            "\n",
            "Epoch 16 Loss 0.0215\n",
            "Time taken for 1 epoch 117.73 sec\n",
            "\n",
            "Epoch 17 Loss 0.0210\n",
            "Time taken for 1 epoch 117.30 sec\n",
            "\n",
            "Epoch 18 Loss 0.0207\n",
            "Time taken for 1 epoch 117.70 sec\n",
            "\n",
            "Epoch 19 Loss 0.0199\n",
            "Time taken for 1 epoch 117.11 sec\n",
            "\n",
            "Epoch 20 Loss 0.0201\n",
            "Time taken for 1 epoch 117.34 sec\n",
            "\n",
            "Train loss:  [0.3845534614894701, 0.13504669631736865, 0.10000274215919384, 0.07791390902754189, 0.06044227489526721, 0.049154469586800836, 0.03748424087745556, 0.03344020290651183, 0.02979615805805593, 0.027536428147468015, 0.025514016635176065, 0.025825613823489867, 0.025812787595002547, 0.02245937706767649, 0.021864729342253312, 0.021512039847995924, 0.020982514257016388, 0.020713139962458955, 0.01990462316982988, 0.020067029759503793]\n",
            "Validation Accuracy:  0.4112921735138857\n",
            "Test Accuracy:  0.3968007109531215\n",
            "\n",
            " Input: \tpharmacy\n",
            "\n",
            "Predicted transliteration: फर्मेसी\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 9 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 9 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output character: फ \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>\t </text><text style=color:#000;background-color:#89c4e2>p </text><text style=color:#000;background-color:#f45f5f>h </text><text style=color:#000;background-color:#89c4e2>a </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>m </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>c </text><text style=color:#000;background-color:#85c2e1>y </text><text style=color:#000;background-color:#85c2e1>\n",
              " </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output character: र \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>\t </text><text style=color:#000;background-color:#85c2e1>p </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#f42e2e>r </text><text style=color:#000;background-color:#85c2e1>m </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>c </text><text style=color:#000;background-color:#85c2e1>y </text><text style=color:#000;background-color:#85c2e1>\n",
              " </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output character: ् \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>\t </text><text style=color:#000;background-color:#85c2e1>p </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#f42e2e>m </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>c </text><text style=color:#000;background-color:#85c2e1>y </text><text style=color:#000;background-color:#85c2e1>\n",
              " </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output character: म \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>\t </text><text style=color:#000;background-color:#85c2e1>p </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>m </text><text style=color:#000;background-color:#89c4e2>a </text><text style=color:#000;background-color:#f8a8a8>c </text><text style=color:#000;background-color:#99cce6>y </text><text style=color:#000;background-color:#85c2e1>\n",
              " </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output character: े \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>\t </text><text style=color:#000;background-color:#85c2e1>p </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>m </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#f9bdbd>c </text><text style=color:#000;background-color:#baddee>y </text><text style=color:#000;background-color:#85c2e1>\n",
              " </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output character: स \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>\t </text><text style=color:#000;background-color:#85c2e1>p </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>m </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>c </text><text style=color:#000;background-color:#f33b3b>y </text><text style=color:#000;background-color:#89c4e2>\n",
              " </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output character: ी \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>\t </text><text style=color:#000;background-color:#85c2e1>p </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>m </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>c </text><text style=color:#000;background-color:#f9e8e8>y </text><text style=color:#000;background-color:#f9e8e8>\n",
              " </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output character: \n",
            " \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<text style=color:#000;background-color:#85c2e1>\t </text><text style=color:#000;background-color:#85c2e1>p </text><text style=color:#000;background-color:#85c2e1>h </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>r </text><text style=color:#000;background-color:#85c2e1>m </text><text style=color:#000;background-color:#85c2e1>a </text><text style=color:#000;background-color:#85c2e1>c </text><text style=color:#000;background-color:#99cce6>y </text><text style=color:#000;background-color:#f68f8f>\n",
              " </text>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tagr\n",
            "\n",
            "Predicted transliteration: अग्र\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tmahanatam\n",
            "\n",
            "Predicted transliteration: महानतम\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMYAAACRCAYAAACYEMcSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAIzklEQVR4nO3db4xcZRXH8e9v29KFhhIraqukfzQSfKEQ0rTViC2BxCZCwAQDxCAtyhqlBt+YaKIJ8QUYRYMJiDQkhWoMMRgb8YVEKgVMWpLaNFpbKjEWy9+2bkvrtt3dtscXczfZrs/O3rl37tyZ3d8nmaTz58w5nenpfe6d5z5XEYGZna+v7gLMupEbwyzBjWGW4MYwS3BjmCW4McwS3Bg5Sdom6eG667DOmN3phJJWA48BpxNPvxIRt3W4JGtC0jZgT0RsqLuWTup4YwAXAk9FxH3jH5TUD/yhhnrM/k9PDKWyYcyjkn4saVDSYUn3Spor6RFJxyT9W9IdU7zPWkkvSTqavc+zkj7WQil9ku6XdETSIUkPSsr1GRbNnf3df9bpvFnsE8Bq4B5Jkd2W5oktkrtd33M79ERjZL4InABWAj8AHgK2AP8AlgNPAo9LWtTkPeZlcSuANcC7wDOSLmihhjPAp4ANwDeBW3PGlsldV957ge3AJmBRdjuYM2/R3O34nsuLiI7egLXAfYnH+4Ftk8RsA7aPuy/gMPC7cY/NAUaAW1qoZR5wFvh0jteeV0P22B+Bxwt+Drly15V3Qv6H2/TdN81d1fdc5NZLW4y/jv0hGp/QIeBv4x4bBY4C75/sDSR9RNKvJP1T0nHgHRpbzcWt1pB5s1m+NuauK28pBXOX/p7boY6d76JGJ9yPSR5r1uy/B14Hvgq8QWN4shfIO5RqNV+7cteVt6wiudvxPZfWS41RiqT3AlcAX4+I57PHrqYDn0FduduUdwSYVVPu2vREkW1yFDgC3C3pIPAh4Ec0/hebrrnbkfcAsCI7GvVfYDAiznUod216aR+jlOzLvBX4BLAHeAT4HjA8XXO3Ke+DNLYae2nsCOfaN6nz824HZXv6nUsorQVWxSQ/8EXEmo4WZJYwY7YYZq2oYx/jXeAGSTcknvtLp4sxS+n4UMqsF3goZZbgxjBLcGOYJdTaGJIG6oqfabF15u7F2Lq3GKW+6JLxMy22ztw9F1t3Y5h1pcoP116gudHPvORzowwzh7mTxg4vu6jpe589PsSs+en3Bug7pUmfO3NyiNkXTR475+2hSZ+bqu5m6oqtM3e3xp5miJEYTv4jqfwHvn7msVLXFYp99ftXl8p94Sv9hWMve2B78cT+bagnvBxbJ33OQymzhKaNIWnVuJPgm922dKpgs05o2hgRsSMiFBECXgDWj93PHnsNuDYibu5EsWad4qGUWYIbwyyhkqNS2S+OAwD9ND/kataNptxiSFomaZTGinSbxu90A0tSMRGxMSKWR8TyMsfdzeqSZyh1ANiR/XldYufbbNqZsjGyRa9uAjYD+yuvyKwL5NrHiIhB4M6KazHrGj4qZZZQ6qhURCxtUx1mXaWrVyL86Jd21Zb72Td3F4797AevamMlVgcPpcwS3BhmCW4Ms4Q8v3yv8bRzm2mm3PmOiG00Lvl0HkkLgbeyX8DNppXcQylJ92RX3xybJ/VWhXWZ1aqVfYwNwMD4uVLeWth01UpjHAH+U1UhZt2klcb4JfBTSf+SdFrSPknJ+VOSBiTtlLRztDcuoGN2nlYa4xIaV8/8HDAfuAt4SNIVE1/o8zGs17XSGJ8BHouIvRExAuwEBoHLKqnMrEatNMY7NC5PO+Z5Gvscf25rRWZdoJXGeJTGpWmvz+5/C7gmIk63vyyzeuVujIjYCdwB/FrSD4H9EeE9a5uWWporFRG/Ba4EFgA/qaQisy7Q8vkYEXEQ+EoFtXSVtYuXF46dNb/4kkHf3v1i4ViAB668pnBsnDpVPPbMmcKx3ciza80S3BhmCYUaQ9JVE6adv93uwszqVKgxImJ3NoHwSeA7EbGwvWWZ1ctDKbMEN4ZZghvDLMGXATBLqGSL4Wnn1us8lDJLcGOYJbgxzBLKrna+rk11mHUVbzHMErr6MgB1KjONOkZGCsd++emvFY4FWPbxk4Vjj11e/ND6e57YXjiWvlnFYwHOnS0Xn+AthlmCG8MsIc9q59fnWO38qU4Ua9YpeVY7f47EaucAkg4At0XEjtTzZr3KQymzBDeGWUKuxshOZR2duG8BLKm4PrNa5N1ivAocAo5NuDbGa6kXe7Vz63W5fuCLiKFsac4bc75+I7ARYL4WRPHyzOqR+5fviNgH7KuwFrOu4Z1vswQ3hllC2WnnS9tUh1lX8RbDLEER1R40mq8FsVLXVZpjOtHscmcCzFr4gcKxd/3ppcKxGy//cOHYvosvLhwLcO7EiUJxL8dWjsdgcrqTtxhmCW4MswQ3hllCu87H8LRzm1ambIyIeG78/KgJc6VeAG6PiFXVl2rWOR5KmSXkbgxJN0r6u6ThcdPOV1dYm1lt8p6P0Qf8AtgMLJgwlEq93tPOrafl3WJcClwC/DwihqZ6sVc7t16XtzEOAyfxGXs2Q+RqjGjMG3ka+Ea15Zh1h1aOSn0XuEXS+qqKMesWuRsjIg4CNwH3S1pUXUlm9WtpKmdEvChpaUQMZ/fXVFKVWc1anuM81hRWjTKrrAOcef2NwrFlpo6XWbH8N/u2Fs8LfH7xJ4sFNlkk3b98myW4McwS3BhmCU0bQ9KqHFPOQ9KWThVs1glNGyMidkyYF7U+sUTntRFxcyeKNesUD6XMEtwYZgmVXLVV0gAwANBP8SuBmtUlzznfyySN0jgpaVOe62N42rn1ujxDqQPA2GIH6/JcH8Os1+VZDCFoTB7cDOyvvCKzLpD3wjGDwJ0V12LWNXxUyizBlwEwS/AWwyyhkt8xzFpx8xfuLhW/YteuQnF7bp/8EhjeYpgluDHMEtwYZgluDLMEN4ZZghvDLMGNYZbg8zHMEirZYvh8DOt1HkqZJbgxzBLcGGYJbgyzBDeGWYIap3RXmEA6zOSLJlwKHCnx9mXiZ1psnbm7NXZJRLwv+UxE1HYDdtYVP9Nie7XuumI9lDJLcGOYJdTdGBtrjJ9psXXm7rnYyne+zXpR3VsMs67kxjBLcGOYJbgxzBLcGGYJ/wMkXmjTOGJKXgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tjoddta\n",
            "\n",
            "Predicted transliteration: जोडता\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tkatthaii\n",
            "\n",
            "Predicted transliteration: कट्ठाई\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tvishahin\n",
            "\n",
            "Predicted transliteration: विषाहिन\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tbolane\n",
            "\n",
            "Predicted transliteration: बोलने\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tnikalegi\n",
            "\n",
            "Predicted transliteration: निकालेगी\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tmamuli\n",
            "\n",
            "Predicted transliteration: मामुली\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input: \tbhagane\n",
            "\n",
            "Predicted transliteration: भागने\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZfLraCi9tER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "67f172f9-3676-40b5-fa34-c8d872d25849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/predictions_attention/ (stored 0%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/ (stored 0%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ibolane^J.png (deflated 11%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ipharmacy^J.png (deflated 8%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/connectivity.html (deflated 96%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ibhagane^J.png (deflated 12%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ijoddta^J.png (deflated 11%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Inikalegi^J.png (deflated 8%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Imahanatam^J.png (deflated 10%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Iagr^J.png (deflated 13%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ikatthaii^J.png (deflated 9%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/success.txt (deflated 79%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/failure.txt (deflated 76%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Imamuli^J.png (deflated 12%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ivishahin^J.png (deflated 9%)\n",
            "  adding: content/training_checkpoints/ (stored 0%)\n",
            "  adding: content/training_checkpoints/ckpt-7.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-5.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-10.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-9.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-1.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-4.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-9.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-8.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/checkpoint (deflated 38%)\n",
            "  adding: content/training_checkpoints/ckpt-8.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-10.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-7.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-4.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-3.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-2.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-6.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-3.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-2.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-6.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-1.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-5.index (deflated 69%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_61960e6f-795e-4675-81cd-591bab762e1c\", \"predictions_attention.zip\", 84504)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_72c2bca5-bde4-43d0-b21f-28fd7c401137\", \"training_checkpoints.zip\", 2119938640)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Download a copy of the predictions_attention and training_checkpoints folder.\n",
        "!zip -r /content/predictions_attention.zip /content/predictions_attention\n",
        "!zip -r /content/training_checkpoints.zip /content/training_checkpoints\n",
        "from google.colab import files\n",
        "files.download(\"/content/predictions_attention.zip\")\n",
        "files.download(\"/content/training_checkpoints.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fdPIgZbocyn4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cs6910_assignment3_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}